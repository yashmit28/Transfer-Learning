{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
    "img_rows = 224\n",
    "img_cols = 224 \n",
    "      \n",
    "#Loads the VGG16 model \n",
    "model = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the VGG16 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpsecting each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D True\n",
      "2 Conv2D True\n",
      "3 MaxPooling2D True\n",
      "4 Conv2D True\n",
      "5 Conv2D True\n",
      "6 MaxPooling2D True\n",
      "7 Conv2D True\n",
      "8 Conv2D True\n",
      "9 Conv2D True\n",
      "10 MaxPooling2D True\n",
      "11 Conv2D True\n",
      "12 Conv2D True\n",
      "13 Conv2D True\n",
      "14 MaxPooling2D True\n",
      "15 Conv2D True\n",
      "16 Conv2D True\n",
      "17 Conv2D True\n",
      "18 MaxPooling2D True\n"
     ]
    }
   ],
   "source": [
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's freeze all layers except the top 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
    "img_rows = 224\n",
    "img_cols = 224 \n",
    "\n",
    "# Re-loads the VGG16 model without the top or FC layers\n",
    "model = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n",
    "\n",
    "# Here we freeze the last 4 layers \n",
    "# Layers are set to trainable as True by default\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a function that returns our FC Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model, num_classes, D=256):\n",
    "    \"\"\"creates the top or head of the model that will be \n",
    "    placed ontop of the bottom layers\"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = Flatten(name = \"flatten\")(top_model)\n",
    "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(None, 224, 224, 3) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x2c0cc2886a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc288710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc2889b0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2c0cc288eb8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc288d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc296898>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2c0c47d6320>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0c47d69b0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc29dfd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc29e518>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2c0cc29ee10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc29ec18>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc2ad7f0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc2ade10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2c0cc2b3b00>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc2b3908>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc2bd4e0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2c0cc2bdeb8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2c0cc3f47f0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add our FC Head back onto VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 21,140,042\n",
      "Trainable params: 6,425,354\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "FC_Head = addTopModel(model, num_classes)\n",
    "\n",
    "modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
    "\n",
    "print(modelnew.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 395 images belonging to 10 classes.\n",
      "Found 52 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = '10_Personality/train/'\n",
    "validation_data_dir = '10_Personality/val/'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 16\n",
    "val_batchsize = 10\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "74/74 [==============================] - 356s 5s/step - loss: 3.2306 - accuracy: 0.2609 - val_loss: 2.6981 - val_accuracy: 0.1630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.69807, saving model to 10_personality.h5\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 349s 5s/step - loss: 1.6568 - accuracy: 0.4465 - val_loss: 2.3447 - val_accuracy: 0.1548\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.69807 to 2.34472, saving model to 10_personality.h5\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 353s 5s/step - loss: 1.3532 - accuracy: 0.5432 - val_loss: 4.4242 - val_accuracy: 0.2024\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.34472\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 369s 5s/step - loss: 1.1460 - accuracy: 0.6185 - val_loss: 1.9008 - val_accuracy: 0.3043\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.34472 to 1.90083, saving model to 10_personality.h5\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 379s 5s/step - loss: 0.9512 - accuracy: 0.6801 - val_loss: 3.5031 - val_accuracy: 0.3571\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.90083\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 353s 5s/step - loss: 0.8543 - accuracy: 0.7117 - val_loss: 4.9223 - val_accuracy: 0.2976\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.90083\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 349s 5s/step - loss: 0.7466 - accuracy: 0.7382 - val_loss: 1.3089 - val_accuracy: 0.4348\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.90083 to 1.30893, saving model to 10_personality.h5\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 350s 5s/step - loss: 0.6322 - accuracy: 0.7707 - val_loss: 2.5945 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.30893\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 380s 5s/step - loss: 0.6260 - accuracy: 0.7836 - val_loss: 3.2277 - val_accuracy: 0.4048\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.30893\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 435s 6s/step - loss: 0.5685 - accuracy: 0.8160 - val_loss: 1.1570 - val_accuracy: 0.4565\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.30893 to 1.15699, saving model to 10_personality.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python import keras\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "                   \n",
    "checkpoint = ModelCheckpoint(\"10_personality.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint]\n",
    "\n",
    "# Note we use a very small learning rate \n",
    "modelnew.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 1190\n",
    "nb_validation_samples = 170\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "history = modelnew.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)\n",
    "\n",
    "model.save(\"10_personality.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_testing=[\"10_Personality/val/dwayne_johnson/a1.jpg\",\n",
    "               \"10_Personality/val/keanu_reeves/b1.jpg\",\n",
    "               \"10_Personality/val/will_smith/c1.jpg\",\n",
    "               \"10_Personality/val/jerry_seinfeld/d1.jpg\",\n",
    "               \"10_Personality/val/kate_beckinsale/e1.jpg\",\n",
    "               \"10_Personality/val/simon_pegg/f1.jpg\",\n",
    "               \"10_Personality/val/arnold_schwarzenegger/arsr.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Flatten\n",
    "#model.Flatten()\n",
    "classifier = load_model('10_personality.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgs = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(result[0][1])\n",
    "def show_img(name,path_f):\n",
    "    import cv2\n",
    "    \n",
    "    imgshow = cv2.imread(path_f) \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "\n",
    "    # org\n",
    "    org = (25, 25) \n",
    "\n",
    "    # fontScale \n",
    "    fontScale = 0.5\n",
    "\n",
    "    # Blue color in BGR \n",
    "    color = (255, 0, 0) \n",
    "\n",
    "    # Line thickness of 2 px \n",
    "    thickness = 1\n",
    "\n",
    "    # Using cv2.putText() method \n",
    "    image_f = cv2.putText(imgshow, name, org, font,  \n",
    "                       fontScale, color, thickness, cv2.LINE_AA) \n",
    "\n",
    "    # Displaying the image \n",
    "    cv2.imshow('test', image_f)  \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_m(result,path):\n",
    "    \n",
    "    if result[0][0] >= 0.9:\n",
    "        imgs = 'will_smith'\n",
    "        show_img(imgs ,path) \n",
    "    elif result[0][1] >= 0.9:\n",
    "        imgs = 'ben_afflek'\n",
    "        show_img(imgs ,path) \n",
    "    elif result[0][2] >= 0.9:\n",
    "        imgs = 'dwayne_johnson'\n",
    "        show_img(imgs ,path) \n",
    "    elif result[0][3] >= 0.9:\n",
    "        imgs = 'jerry_seinfeld'\n",
    "        show_img(imgs ,path) \n",
    "    elif result[0][4] >= 0.9:\n",
    "        imgs = 'kate_beckinsale'\n",
    "        show_img(imgs ,path) \n",
    "    elif result[0][5] >= 0.9:\n",
    "        imgs = 'keanu_reeves'\n",
    "        show_img(imgs ,path) \n",
    "    elif result[0][6] >= 0.9:\n",
    "        imgs = 'madonna'\n",
    "        show_img(imgs ,path)\n",
    "    elif result[0][7] >= 0.9:\n",
    "        imgs = 'simon_pegg'\n",
    "        show_img(imgs,path)\n",
    "    elif result[0][8] >= 0.9:\n",
    "        imgs = 'sofia_vergara'\n",
    "        show_img(imgs,path)\n",
    "    elif result[0][9] >= 0.9:\n",
    "        imgs = 'arnold_schwarzenegger'\n",
    "        show_img(imgs,path)\n",
    "    else:\n",
    "        imgs = \"I dont know, cool\"\n",
    "        show_img(imgs,path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np \n",
    "#def pr_f(i):\n",
    "    \n",
    "test_image = image.load_img(\"10_Personality/val/arnold_schwarzenegger/arsr.jpg\", \n",
    "               target_size=(224,224))\n",
    "    #print(test_image)\n",
    "    #print(model_testing[i])\n",
    "    #img_show = test_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pr_f(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.expand_dims(test_image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(test_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_m(result, \"10_Personality/val/arnold_schwarzenegger/arsr.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
